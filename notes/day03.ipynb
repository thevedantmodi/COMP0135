{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW0\n",
    "\n",
    "use [q:q+1] to get the (1, F) shape rather than [q] to get the (F, ) shape\n",
    "\n",
    "Be sure to vectorize the numpy object , don't write nested for loops\n",
    " > Great performance improvements\n",
    "\n",
    "Try to start with for loops then move to vectorize \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Recall: MSE (mean square error)\n",
    "\n",
    "Will be smooth everywhere (differentiable), unlike MAE (mean absolute error)\n",
    "\n",
    "Goal: Minimize squared error\n",
    "\n",
    "Objective function (J) - something we want to minimize\n",
    "\n",
    "min_(w) : means want to minimize objective w.r.t w (find the lowest w)\n",
    "\n",
    "J'(w) = 0 (find the critical point)\n",
    "\n",
    "Can use rise over run\n",
    "\n",
    "Need to minimize the w by taking the derivative of the objective function w.r.t\n",
    "to w, and solving for w when J' is 0\n",
    "\n",
    "When training for 1-dim no bias Linear regression, need 1 xi to be non-zero (or\n",
    "else zero in denominator)\n",
    "\n",
    "\n",
    "Theta = [w1 .. wf b]\n",
    "~xn = [xn1 .. xnF 1]\n",
    "\n",
    "Theta^T * ~xn is just dot product (inner product)\n",
    "\n",
    "In \"always holds\", we have (F+1, F+1) * (F+1, 1) = (F+1, N) * (N, 1)\n",
    "(dimensionality agrees)\n",
    "\n",
    "if inverse exists: we have that Theta  has at least F + 1 feature vectors that\n",
    "are linearly independent\n",
    "\n",
    "Otherwise, many w, b will yield lowest possible training error\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs135_25s_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
