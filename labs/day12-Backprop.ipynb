{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 135 day12: Backpropagation\n",
    "\n",
    "# Objectives\n",
    "\n",
    "* Walk through backpropagation demo\n",
    "* Manually try backpropagation for a sample input\n",
    "* Compare your result to the implementation of backprop in sklearn\n",
    "\n",
    "# Outline\n",
    "\n",
    "* [Part 1: Backprop demo from Google's ML crash course](#part1)\n",
    "* [Part 2: Manual do forward and backward pass on simple input](#part2)\n",
    "* [Part 3: Compare Part 2 results with sklearn implementation](#part3)\n",
    "\n",
    "We expect you'll get through at least first 2 parts during the in-class session.\n",
    "\n",
    "\n",
    "# Takeaways\n",
    "\n",
    "* Backpropagation is a way to compute the gradients of composable functions\n",
    "* Running backpropagation requires:\n",
    "* --- Allocating storage at each node in computation graph\n",
    "* --- Running forward pass to compute inputs and outputs of each node\n",
    "* --- Running backward pass to compute gradients wrt each node's inputs and outputs, using the chain rule\n",
    "* --- Computing each parameter's gradients using results of backward pass\n",
    "* Sklearn uses backpropagation within its `MLPRegressor` and `MLPClassifier` implementations to compute the gradients it uses in its gradient descent algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8') # pretty matplotlib plots\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('notebook', font_scale=1.25, style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Read through the backprop demo from Google's ML crash course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo considers a simple neural net that is like a general MLP, with\n",
    "\n",
    "* hidden_layer_sizes = [2, 2]\n",
    "* No bias parameters (they are fixed to zero and never updated)\n",
    "* general purpose activation function\n",
    "\n",
    "Open this link in your browser, and scroll through each step\n",
    "\n",
    "<https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 1a: Which update steps of the backward pass use the chain rule?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 1b: What would you need to change in the provided computations to do backprop for binary classification with log loss instead of regression with squared error loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Try out manually computing the gradient using backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following *simpler* network architecture, which is like the demo above but only uses one hidden layer (not two)\n",
    "\n",
    "* Terminal node loss function: $\\frac{1}{2}(\\hat{y}_{\\text{output}} - y_{\\text{target}})^2$\n",
    "* Hidden layer sizes: [2]\n",
    "* Hidden activation: ReLU\n",
    "\n",
    "\n",
    "You might need these elementary derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d y} \\frac{1}{2}( y - m)^2  = y - m\n",
    "$$\n",
    "\n",
    "$$\\frac{d}{d x} \\text{ReLU}(x) =\n",
    "\\begin{cases}\n",
    "    1 \\quad \\text{if} ~ x > 0 \\\\\n",
    "    0 \\quad \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider running the algorithm with the following inputs:\n",
    "    \n",
    "### Data:\n",
    "    \n",
    "* $x_{\\text{in}} = 1.0$\n",
    "* $y_{\\text{target}} = 1.0$\n",
    "\n",
    "### Parameters for Layer 1 (input-to-hidden):\n",
    "\n",
    "First neuron:\n",
    "\n",
    "* $w_{11} = 1.0$\n",
    "* $b_{11} = 0.0$\n",
    "\n",
    "Second neuron:\n",
    "\n",
    "* $w_{12} = 2.0$\n",
    "* $b_{12} = 0.0$\n",
    "\n",
    "### Parameters for Layer 2 (hidden-to-output):\n",
    "\n",
    "* $w_{21} = 1.0$\n",
    "* $w_{22} = 1.0$\n",
    "* $b_2 = 0.0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2a: Forward pass\n",
    "\n",
    "What is the *output* of each node? Work it out manually and write your answer below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "* Layer 1 Neuron 1 output: ____\n",
    "* Layer 1 Neuron 2 output: ____\n",
    "* Layer 2 Output: ____\n",
    "\n",
    "* Terminal node output:  ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2b: Backward pass for layer 2\n",
    "\n",
    "What is gradient of E wrt the Layer 2 output?\n",
    "\n",
    "What is gradient of E wrt the Layer 2 input?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2c: Backward pass for layer 1\n",
    "\n",
    "What is gradient of E wrt each neuron's output in Layer 1?\n",
    "\n",
    "What is gradient of E wrt each neuron's input in Layer 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2d: What is gradient wrt each weight parameter in Layer 2?\n",
    "\n",
    "You can skip the bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2e: What is gradient wrt each weight parameter in Layer 1?\n",
    "\n",
    "You can skip the bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Check your Part 2 answer with the result of sklearn\n",
    "\n",
    "We can use sklearn to compute every result you did in Part 2.\n",
    "\n",
    "Use this section to check your answer, and get a quick understanding of how sklearn's MLP handles forward and backward passes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an example input-output data pair\n",
    "\n",
    "* x is a scalar, but we reshape to (1,1) to meet sklearn's 2-dim requirement for input arrays\n",
    "* y is a scalar, but we reshpae to (1,) to meet sklearn's 1-dim requirement for output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_11 = np.asarray([1.0]).reshape(1,1)\n",
    "y_1 = np.asarray([1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the MLP object and initialize it for the give problem\n",
    "\n",
    "We set max_iter = 1 so that fit really just initializes all arrays to the correct size, \n",
    "and then quits.\n",
    "\n",
    "We fully expect a \"Convergence Warning\". You can just ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = sklearn.neural_network.MLPRegressor(\n",
    "    hidden_layer_sizes=[2],\n",
    "    activation='relu',\n",
    "    alpha=0.0, max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mhughes/micromamba/envs/cs135_25s_env/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(x_11, y_1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's fill in our weights with desired values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer\n",
    "\n",
    "mlp.coefs_[0][:] = np.asarray([[1.0, 2.0]])\n",
    "\n",
    "mlp.intercepts_[0][:] = np.asarray([0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second layer \n",
    "\n",
    "mlp.coefs_[1][:] = np.asarray([[1.0], [1.0]])\n",
    "\n",
    "mlp.intercepts_[1][:] = np.asarray([0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate storage for forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.n_layers_ # num layers includes input, hidden, and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Initialize storage for forward pass\n",
    "# This is a list with one entry for each layer (including input layer)\n",
    "outputs_per_node = [None for layer_id in range(mlp.n_layers_)]\n",
    "\n",
    "print(len(outputs_per_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_per_node[0] = 1.0 * x_11 # Copy x_11 input data into input layer slot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the forward pass\n",
    "\n",
    "Unusually, the terminal node (loss node) is not computed here.\n",
    "\n",
    "Instead, that's done with backward pass below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, first layer's \"output\" is just input x_11, so that's being fed in implicitly\n",
    "# Unusually, sklearn does not compute the terminal node output here, that's done below\n",
    "outputs_per_node = mlp._forward_pass(outputs_per_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate storage for backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place to store gradients of loss wrt each node's input\n",
    "# This is a list for each non-input layer\n",
    "deltas_per_node = [[None] for layer_id in range(mlp.n_layers_ - 1)]\n",
    "\n",
    "# Place to store gradients of loss wrt each weight and bias\n",
    "w_grads = [np.zeros_like(w_arr) for w_arr in mlp.coefs_]\n",
    "b_grads = [np.zeros_like(b_arr) for b_arr in mlp.intercepts_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do final loss computation plus backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_node_loss, w_grads, b_grads = mlp._backprop(\n",
    "    x_11, y_1,\n",
    "    outputs_per_node, deltas_per_node,\n",
    "    w_grads, b_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3a: Show results of forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of layer 1:\n",
      "[[1. 2.]]\n",
      "Output of layer 2:\n",
      "[[3.]]\n",
      "Output of terminal node:\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Output of layer 1:\")\n",
    "print(outputs_per_node[1])\n",
    "\n",
    "print(\"Output of layer 2:\")\n",
    "print(outputs_per_node[2])\n",
    "\n",
    "print(\"Output of terminal node:\")\n",
    "print(terminal_node_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 3b: Show results of grad wrt inputs of Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is gradient of E wrt each neuron's input value in Layer 2?\n",
      "[[2.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"What is gradient of E wrt each neuron's input value in Layer 2?\")\n",
    "print(deltas_per_node[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 3c: Show results of grad wrt inputs of Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is gradient of E wrt each neuron's input value in Layer 1?\n",
      "[[2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"What is gradient of E wrt each neuron's input value in Layer 1?\")\n",
    "print(deltas_per_node[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 3d: Show results of grad wrt WEIGHTS for Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad wrt weight coefs for layer 2\n",
      "[[2.]\n",
      " [4.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Grad wrt weight coefs for layer 2\")\n",
    "print(w_grads[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 3e: Show results of grad wrt WEIGHTS for Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad wrt weight coefs for layer 1\n",
      "[[2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Grad wrt weight coefs for layer 1\")\n",
    "print(w_grads[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Result: Show results of grad wrt BIAS PARAMETERS for Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad wrt intercept coefs for layer 1\n",
      "[2. 2.]\n",
      "\n",
      "Grad wrt intercept coefs for layer 2\n",
      "[2.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer_id in range(mlp.n_layers_ - 1):\n",
    "    print(\"Grad wrt intercept coefs for layer %d\" % (layer_id+1))\n",
    "    print(b_grads[layer_id])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
